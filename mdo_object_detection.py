# -*- coding: utf-8 -*-

# @title Imports and function definitions

# For running inference on the TF-Hub module.
import tensorflow as tf

import tensorflow_hub as hub

# For downloading the image.
import matplotlib.pyplot as plt
import tempfile
from six.moves.urllib.request import urlopen
from six import BytesIO

# For drawing onto the image.
import numpy as np
from PIL import Image
from PIL import ImageColor
from PIL import ImageDraw
from PIL import ImageFont
from PIL import ImageOps

# For measuring the inference time.
import time

# for grabbing arguments and ordinary stuff
import argparse
import os
import sys


"""mdo_object_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tckTCfmXxBBw1pKcM5WdFLBIhIA8VAf9

##### Copyright 2018 The TensorFlow Hub Authors.

Licensed under the Apache License, Version 2.0 (the "License");
"""

# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

# Apologies from Mark-MDO47 - For my own education and amusement I am
# using this notebook as a starting point to learn more about TensorFlow 2.x.
#
# Any modifications made to the original are mine and mine alone and
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
#
# In reality, any changes I make are probably not very good and not of interest
# to anyone other than myself.

MIN_SCORE=0.1

"""# Object detection

<table class="tfo-notebook-buttons" align="left">
  <td>
    <a target="_blank" href="https://www.tensorflow.org/hub/tutorials/object_detection"><img src="https://www.tensorflow.org/images/tf_logo_32px.png" />View on TensorFlow.org</a>
  </td>
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Run in Google Colab</a>
  </td>
  <td>
    <a target="_blank" href="https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb"><img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" />View source on GitHub</a>
  </td>
  <td>
    <a href="https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/object_detection.ipynb"><img src="https://www.tensorflow.org/images/download_logo_32px.png" />Download notebook</a>
  </td>
</table>

This Colab demonstrates use of a TF-Hub module trained to perform object detection.

## Setup
"""


"""## Example use

### Helper functions for downloading images and for visualization.

Visualization code adapted from [TF object detection API](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py) for the simplest required functionality.
"""


def display_image(image):
    fig = plt.figure(figsize=(20, 15))
    plt.grid(False)
    plt.imshow(image)


def download_and_resize_image(url, new_width=256, new_height=256,
                              display=False, filename="UNKNOWN"):
    if "UNKNOWN" == filename:
        _, filename = tempfile.mkstemp(suffix=".jpg")
    response = urlopen(url)
    image_data = response.read()
    image_data = BytesIO(image_data)
    pil_image = Image.open(image_data)
    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)
    pil_image_rgb = pil_image.convert("RGB")
    pil_image_rgb.save(filename, format="JPEG", quality=90)
    print("Image downloaded to %s." % filename)
    if display:
        display_image(pil_image)
    return filename


def draw_bounding_box_on_image(image,
                               ymin,
                               xmin,
                               ymax,
                               xmax,
                               color,
                               font,
                               thickness=4,
                               display_str_list=()):
    """Adds a bounding box to an image."""
    draw = ImageDraw.Draw(image)
    im_width, im_height = image.size
    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,
                                  ymin * im_height, ymax * im_height)
    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),
               (left, top)],
              width=thickness,
              fill=color)

    # If the total height of the display strings added to the top of the bounding
    # box exceeds the top of the image, stack the strings below the bounding box
    # instead of above.
    display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]
    # Each display_str has a top and bottom margin of 0.05x.
    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)

    if top > total_display_str_height:
        text_bottom = top
    else:
        text_bottom = top + total_display_str_height
    # Reverse list and print from bottom to top.
    for display_str in display_str_list[::-1]:
        text_width, text_height = font.getsize(display_str)
        margin = np.ceil(0.05 * text_height)
        draw.rectangle([(left, text_bottom - text_height - 2 * margin),
                        (left + text_width, text_bottom)],
                       fill=color)
        draw.text((left + margin, text_bottom - text_height - margin),
                  display_str,
                  fill="black",
                  font=font)
        text_bottom -= text_height - 2 * margin


def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=MIN_SCORE):
    """Overlay labeled boxes on an image with formatted scores and label names."""
    colors = list(ImageColor.colormap.values())

    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf",
                                  25)
    except IOError:
        print("Font not found, using default font.")
        font = ImageFont.load_default()

    for i in range(min(boxes.shape[0], max_boxes)):
        if scores[i] >= min_score:
            ymin, xmin, ymax, xmax = tuple(boxes[i])
            display_str = "{}: {}%".format(class_names[i].decode("ascii"),
                                           int(100 * scores[i]))
            color = colors[hash(class_names[i]) % len(colors)]
            image_pil = Image.fromarray(np.uint8(image)).convert("RGB")
            draw_bounding_box_on_image(
                image_pil,
                ymin,
                xmin,
                ymax,
                xmax,
                color,
                font,
                display_str_list=[display_str])
            np.copyto(image, np.array(image_pil))
    return image


def load_img(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    return img


def run_detector(detector, path):
    img = load_img(path)

    converted_img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]
    start_time = time.time()
    result = detector(converted_img)
    end_time = time.time()

    result = {key: value.numpy() for key, value in result.items()}

    print("Found %d objects." % len(result["detection_scores"]))
    print("Inference time: ", end_time - start_time)

    image_with_boxes = draw_boxes(
        img.numpy(), result["detection_boxes"],
        result["detection_class_entities"], result["detection_scores"])

    display_image(image_with_boxes)
    img_jpg = Image.fromarray(image_with_boxes)
    img_jpg.save(path, format="JPEG", quality=90)
    return result


def show_result(result, min_score=MIN_SCORE, fname="UNKNOWN"):
    print("Results for %s" % fname)
    for idxShow in range(len((result["detection_scores"]))):
        if result["detection_scores"][idxShow] >= min_score:
            print("%s %s %s" % (
            idxShow, result["detection_scores"][idxShow], result["detection_class_entities"][idxShow]))
        else:
            break

def getArgs():
    FILEEXT = ["jpg", "jpeg", "JPG", "JPEG", "png", "PNG"]
    PNGEXT = ["png", "PNG"]
    JPGEXT = ["jpg", "jpeg", "JPG", "JPEG"]
    MODELS=['ssd', 'mobile', 'rcnn', 'incept', 'resnet']
    MOBILE=['ssd', 'mobile']
    INCEPT=['rcnn', 'incept', 'resnet']
    MODELDEFAULT='ssd'

    jpgOrPng = "JPG ("
    for ext in JPGEXT:
        jpgOrPng += "*." + ext + ","
    jpgOrPng = jpgOrPng[:-1] + ')'
    jpgOrPng += " or PNG ("
    for ext in PNGEXT:
        jpgOrPng += "*." + ext + ","
    jpgOrPng = jpgOrPng[:-1] + ')'

    epiloghelp="""Example:
python mdo_object_detection.py {[-d --detector] <<<detector>>>} imagedirectory

Processes images in imagedirectory; outputs *_box.* with bounding boxes
NOTE: only processes %s
""" % jpgOrPng

    detectorhelp = """
ssd or mobile = (default) https://tfhub.dev/google/ssd/mobilenet_v2/1 (small and fast)
rcnn or incept or resnet = https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1 (high accuracy)
"""

    my_parser = argparse.ArgumentParser(prog='mdo_object_detection', epilog=epiloghelp,
        formatter_class=argparse.RawTextHelpFormatter,
        description="makes JPG images with bounding boxes from directory of images",
        usage='%(prog)s {[-d --detector] <<<detector>>>} imagedirectory')
    my_parser.add_argument('imagedirectory',type=str,help='path to directory with images to process')

    my_parser.add_argument('-d', '--detector', nargs=1, help=detectorhelp,
                           choices=('ssd', 'mobile', 'rcnn', 'incept', 'resnet'),
                           default = MODELDEFAULT)

    args = my_parser.parse_args()

    #
    # check that the source directory exists and has image files
    #
    if not os.path.isdir(args.imagedirectory):
        sys.stderr.write("\nERROR: %s is not a source directory of files\n" % args.imagedirectory)
        exit(1)
    theFiles = os.listdir(args.imagedirectory)

    # eliminate files we don't process
    idxF = 0
    while (len(theFiles) > 0) and (idxF < len(theFiles)):
        fnFrag = theFiles[idxF].split(".")
        if fnFrag[-1] not in FILEEXT:
            del theFiles[idxF]
        else:
            idxF += 1
    if len(theFiles) <= 0:
        print("\nERROR: directory %s does not contain any %s files\n" % (args.imagedirectory, jpgOrPng))
        exit(1)
    thePath = os.path.abspath(args.imagedirectory).replace("\\","/") + "/"

    #
    # check that the chosen detector is one we know about
    #
    # @param ["https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1", "https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1"]
    theDetector = "UNKNOWN"
    if args.detector[-1] in MOBILE:
        theDetector = "D:/2020-04-26_TF_models/TensorFlowHub_models/mobilenet_v2"
    elif args.detector[-1] in INCEPT:
        theDetector = "D:/2020-04-26_TF_models/TensorFlowHub_models/inception_resnet_v2"

    return thePath, theFiles, theDetector

if __name__ == "__main__":

    thePath, theFiles, theDetector = getArgs()

    # Print Tensorflow version
    print("Tensorflow version %s" % tf.__version__)

    # Check available GPU devices.
    print("The following GPU devices are available: %s" % tf.test.gpu_device_name())

    detector = hub.load(theDetector).signatures['default']

    for fname in theFiles:
        image_url = "file://localhost/" + thePath + fname
        boxname = thePath + fname
        boxname = boxname[:boxname.rfind(".")]+"_box"+boxname[boxname.rfind("."):]
        downloaded_image_path = download_and_resize_image(image_url, 1280, 856, True, filename=boxname)
        result = run_detector(detector, downloaded_image_path)
        show_result(result, fname=downloaded_image_path)

